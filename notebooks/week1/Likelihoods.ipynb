{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38287ed5-53c1-4a3e-acbb-551a20435541",
   "metadata": {},
   "source": [
    "## Preamble\n",
    "This is designed as a codified version of my notes on BI, and to demonstrate some python implementation of different noise models as likelihood functions. Useful references:\n",
    "\n",
    "[1] [GW BI Introduction Paper](https://arxiv.org/pdf/1809.02293)\n",
    "\n",
    "[2] [LIGO Noise Specs (some more good BI here)](https://arxiv.org/pdf/1908.11170)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c575ccd3-0237-4d4d-98e8-35d17b073d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotfancy as pf\n",
    "import numpy as np\n",
    "pf.housestyle_rcparams()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96631251-0061-4192-89da-8a6fb04c8fa9",
   "metadata": {},
   "source": [
    "## General Gaussian Case ##\n",
    "\n",
    "In the most general case, we relate our observed data $\\textbf{X}$ from some model prediction $\\boldsymbol{\\mu}(\\boldsymbol{\\theta})$, based on model parameters $\\boldsymbol{\\theta}$ such that\n",
    "\\begin{equation}\n",
    "\\textbf{X} = \\boldsymbol{\\mu}(\\boldsymbol{\\theta}) + \\boldsymbol{\\mathscr{N}}\n",
    "\\end{equation}\n",
    "for some noise vector $\\boldsymbol{\\mathscr{N}}$. The most general Gaussian case models $\\boldsymbol{\\mathscr{N}}$ as a multivariate gaussian - a vector whose elements are correlated, encoded with a covariance matrix, $\\mathscr{C}$, relating the variance of each element\n",
    "\\begin{equation}\n",
    "\\mathscr{C} =\n",
    "\\begin{bmatrix}\n",
    "\\sigma^2(1) & \\sigma(1,2) & ... & \\sigma(1,K) \\\\\n",
    "\\sigma(2,1) & \\sigma^2(2) & ... & \\sigma(2,K) \\\\ \n",
    "... & ... & ... & ... \\\\\n",
    "\\sigma(K,1) & \\sigma(K,2) & ... & \\sigma^2(K)\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "for $K = \\texttt{dim}(\\textbf{X})$ and $i,j\\in\\mathbb{Z}^+\\leq K$ for $\\mathscr{C}_{i,j}=\\sigma(i,j)$ covariance. We establish the notation convention here that $\\textbf{X}$, $\\boldsymbol{\\mu}(\\boldsymbol{\\theta})$, and $\\boldsymbol{\\mathscr{N}}$ are *frequency-domain* functions for the data, model and noise hereonwards unless explicitely noted otherwise as time-series.\n",
    "\n",
    "\n",
    "Calculating the likelihood, which by definition follows $\\mathbb{P}(\\textbf{X}|\\boldsymbol{\\theta})$, here is equivalent to some $\\mathbb{P}_{\\boldsymbol{\\mathscr{N}}}(\\textbf{X} - \\boldsymbol{\\mu}(\\boldsymbol{\\theta}))$ conditional probability, which follows from the first equation as our noise distribution;\n",
    "\\begin{equation}\n",
    "\\mathbb{P}(\\textbf{X}|\\boldsymbol{\\theta}) =  \\mathbb{P}_{\\boldsymbol{\\mathscr{N}}}(\\textbf{X} - \\boldsymbol{\\mu}(\\boldsymbol{\\theta})) = \\mathbb{P}_{\\boldsymbol{\\mathscr{N}}}(\\boldsymbol{\\mathscr{N}})\n",
    "\\end{equation}\n",
    "\n",
    "Calculating this in a covariant case is non-trivial - we cannot simply combine probabilities through multiplication - instead we get a multivariate Gaussian of the form\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbb{P}_{MG} = \\frac{1}{[(2\\pi)^K\\textnormal{det}(\\mathscr{C})]^{1/2}} \\exp \\bigg[ -\\frac{1}{2} [\\textbf{X} - \\boldsymbol{\\mu}(\\boldsymbol{\\theta})]^T \\mathscr{C}^{-1} [\\textbf{X} - \\boldsymbol{\\mu}(\\boldsymbol{\\theta})] \\bigg]\n",
    "\\end{equation}\n",
    "\n",
    "We might model this most general case as a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f55ddc-f45d-4774-8fc2-24926a85e7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Global params ###\n",
    "\n",
    "fr = 100 # the number of frequency bins we'd have\n",
    "n = 10 # number of example samples of noise\n",
    "\n",
    "### FIRST FIND COVARIANCE - Observing no signal to create n frequency profiles of the noise ###\n",
    "\n",
    "noise_samples = np.ones([n,fr]) # Dummy array\n",
    "sample_mean = np.average(noise_samples, axis=0)\n",
    "cen = noise_samples - sample_mean\n",
    "cov = 1/(fr) * (np.transpose(cen) @ cen)\n",
    "\n",
    "### We might then test this against some data... ###\n",
    "\n",
    "data = np.ones([1,fr]) \n",
    "model = np.ones([1,fr]) # Dummy arrays we might observe from an experiment.\n",
    "\n",
    "def lhood_MVG(X, MU, C):\n",
    "    n = X-MU\n",
    "    return 1/(2*np.pi*np.linalg.det(C))**(fr/2) * np.exp(-0.5*np.transpose(n)@ np.linalg.inv(C)@ n)\n",
    "\n",
    "### Which would be implemented like ###\n",
    "\n",
    "lhood_MVG(data, model, cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bba4d31-b587-4f15-9de9-57612020494d",
   "metadata": {},
   "source": [
    "### We can, however, simplify in special cases:\n",
    "\n",
    "The next simplest case is stationary noise; using $N(\\mu,\\sigma^2)$ to represent a normal distribution we express this case as noise following\n",
    "\\begin{equation}\n",
    "\\boldsymbol{\\mathscr{N}}(t) \\sim\n",
    "\\begin{bmatrix}\n",
    "N(0,\\sigma^2_1) \\\\ N(0,\\sigma^2_2) \\\\ ... \\\\ N(0,\\sigma^2_K)\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "After the Fourier transform it follows that this then becomes some complex-valued noise - we make the assumption (easily verifiable by plotting the im-real correlation) that the imaginary and real parts are independently stochastic such that\n",
    "\n",
    "\\begin{equation}\n",
    "\\boldsymbol{\\mathscr{N}} = \\mathfrak{Re}(\\boldsymbol{\\mathscr{N}}) + i\\mathfrak{Im}(\\boldsymbol{\\mathscr{N}}),\n",
    "\\end{equation}\n",
    "more verbosely that\n",
    "\\begin{equation}\n",
    "\\boldsymbol{\\mathscr{N}}(f) \\sim\n",
    "\\begin{bmatrix}\n",
    "N(0,\\sigma^2_1) \\\\ N(0,\\sigma^2_2) \\\\ ... \\\\ N(0,\\sigma^2_K)\n",
    "\\end{bmatrix}\n",
    "+ i\n",
    "\\begin{bmatrix}\n",
    "N(0,\\sigma^2_1) \\\\ N(0,\\sigma^2_2) \\\\ ... \\\\ N(0,\\sigma^2_K)\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "in the frequency domain. Note that the real and imaginary elements must be thought as independently sampled from their respective distributions $N(0,\\sigma^2_i)$: this is **not** $\\mathscr{N}_i=N(0,\\sigma^2_i)(1+i)$. This distribution has a diagonal covariance matrix and the frequency domain noise transforms to autocorrelated noise with some characteristic $\\textbf{C}(t_i-t_j)$ correlation function in the time domain. We express $\\sigma^2_i = S_{\\boldsymbol{\\mathscr{N}}}(f_i)$ where $S_{\\boldsymbol{\\mathscr{N}}}(f_i)$ is the power spectral density at some frequency bin $f_i$, estimated by taking $S_n(f_i) = \\texttt{fft}(\\boldsymbol{\\mathscr{N}}(\\tau))$ where $\\tau$ is some characteristic $t_i\\to t_j$ time period and $\\boldsymbol{\\mathscr{N}}(\\tau)$ is the *time-series* noise. For this simplified covariance matrix \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathscr{C} =\n",
    "\\begin{bmatrix}\n",
    "\\sigma^2_1 & 0 & ... & 0 \\\\\n",
    "0 & \\sigma^2_2 & ... & 0 \\\\ \n",
    "... & ... & ... & ... \\\\ \n",
    "0 & 0 & ... & \\sigma^2_K\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "the determinant of this matrix then follows $\\textnormal{det}(\\mathscr{C}) = \\prod^K_i \\sigma_i$; because of the diagonality of the matrix we can further simplify the likelihood. By our assumption of complex-real stochastic independence, it follows that\n",
    "\n",
    "\\begin{aligned}\n",
    "\\textbf{X} &= \\boldsymbol{\\mu}(\\boldsymbol{\\theta}) + \\mathfrak{Re}(\\boldsymbol{\\mathscr{N}}) + i\\mathfrak{Im}(\\boldsymbol{\\mathscr{N}}) \\\\\n",
    "\\Rightarrow \\mathbb{P}(\\textbf{X}|\\boldsymbol{\\theta}) &=  \\mathbb{P}_{\\boldsymbol{\\mathscr{N}}}(\\textbf{X} - \\boldsymbol{\\mu}(\\boldsymbol{\\theta})) \\\\\n",
    "&= \\mathbb{P}_{\\mathfrak{Re}(\\boldsymbol{\\mathscr{N}})}(\\textbf{X} - \\boldsymbol{\\mu}(\\boldsymbol{\\theta})) \\cdot \\mathbb{P}_{\\mathfrak{Im}(\\boldsymbol{\\mathscr{N}})}(\\textbf{X} - \\boldsymbol{\\mu}(\\boldsymbol{\\theta})) \\\\\n",
    "&= \\prod^K_i \\frac{1}{2\\pi\\sigma^2_i} \\exp \\bigg[ \\frac{\\mathfrak{Re}(\\boldsymbol{\\mathscr{N}}_i)^2}{2\\pi\\sigma^2_i} \\bigg] \\cdot \\exp \\bigg[ \\frac{\\mathfrak{Im}(\\boldsymbol{\\mathscr{N}}_i)^2}{2\\pi\\sigma^2_i} \\bigg]  \\\\\n",
    "&= \\prod^K_i \\frac{1}{2\\pi\\sigma^2_i} \\exp \\bigg[ \\frac{\\mathfrak{Re}(\\boldsymbol{\\mathscr{N}}_i)^2 + \\mathfrak{Im}(\\boldsymbol{\\mathscr{N}}_i)^2}{2\\pi\\sigma^2_i} \\bigg]\n",
    "\\end{aligned}\n",
    "\n",
    "Noticing the encoded modulus-squared definition in the exponent argument\n",
    "\\begin{equation}\n",
    "\\mathfrak{Re}(\\boldsymbol{\\mathscr{N}}_i)^2 + \\mathfrak{Im}(\\boldsymbol{\\mathscr{N}}_i)^2 \\equiv |X - \\mu(\\boldsymbol{\\theta})|^2_i\n",
    "\\end{equation}\n",
    "\n",
    "we get to a simplified likelihood of\n",
    "\\begin{equation}\n",
    "\\mathbb{P}_{\\textnormal{stat}} = \\frac{1}{[\\prod^K_i 2\\pi\\sigma_i]^{1/2}} \\exp \\bigg[ -\\frac{1}{2} \\sum^K_i \\frac{[X-\\mu(\\boldsymbol{\\theta})]^2_i}{\\sigma^2_i} \\bigg]\n",
    "\\end{equation}\n",
    "\n",
    "Given this is now separable we can turn this into a *log likelihood* that follows\n",
    "\\begin{equation}\n",
    "\\log[\\mathbb{P}_{\\textnormal{stat}}(\\textbf{X}|\\boldsymbol{\\theta})] = \\sum^K_i\\bigg[\\frac{1}{2\\pi\\sigma_i} [X-\\mu(\\boldsymbol{\\theta})]^2_i - \\frac{1}{2} \\log[{2\\pi\\sigma_i^2}]\\bigg]\n",
    "\\end{equation}\n",
    "\n",
    "Note that in literature the $\\frac{1}{2} \\log[{2\\pi\\sigma_i^2}]$ term is often omitted as it cancels under addition of likelihoods to create a (log) Bayes factor. With the exception of *glitches*, (short duration transients) and *adiabatic drift* (minute-to-hour drifts in the power spectrum), the noise at LIGO can be approximated as stationary [2]. We might implement this case as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37a5f59-896c-4733-982c-01fb7ebc34e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Global params ###\n",
    "\n",
    "fr = 100 # the number of frequency bins we'd have\n",
    "n = 10 # number of example samples of noise\n",
    "\n",
    "### FIRST FIND COVARIANCE - This is simpler as we can just take the variance of each element ###\n",
    "\n",
    "noise_samples = np.ones([n,fr]) # dummy array\n",
    "cov = np.var(noise_samples, axis = 0)*np.identity(fr)\n",
    "\n",
    "### We might then test this against some data... ###\n",
    "\n",
    "data = np.ones([1,fr]) \n",
    "model = np.ones([1,fr]) # dummy arrays we might observe from an experiment.\n",
    "\n",
    "def lhood_stat(X, MU, C):\n",
    "    sigmas = np.diag(C)\n",
    "    n = X-MU\n",
    "    return 1/(np.prod((2*np.pi*sigmas)))**(0.5) * np.exp(-0.5*np.sum((n/sigmas)**2))\n",
    "    \n",
    "def log_lhood_stat(X,MU,C, ret_const=True): # we may also want to define the log-likelihood...\n",
    "    sigmas = np.diag(C)\n",
    "    const = (-0.5*np.sum(np.log((2*np.pi*sigmas**2)))) if ret_const else 0\n",
    "    n = X-MU\n",
    "    return np.sum(n**2/(2*np.pi*sigmas**2)) + const\n",
    "    \n",
    "### Which would be implemented like ###\n",
    "\n",
    "lhood_stat(data, model, cov) #or\n",
    "log_lhood_stat(data, model, cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf586ee-803f-43ea-b3c5-2ed4ef74ad9d",
   "metadata": {},
   "source": [
    "### White Noise\n",
    "\n",
    "The simplest case is white noise: where every element of the noise distribution follows the same normal distribution such that we can express\n",
    "\n",
    "\\begin{equation}\n",
    "\\boldsymbol{\\mathscr{N}} = N(0,\\sigma^2)\n",
    "\\begin{bmatrix}\n",
    "1 \\\\ ... \\\\ 1\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "and the covariance as \n",
    "\\begin{equation}\n",
    "\\mathscr{C} = \\sigma^2 \\textbf{I} = S_{\\boldsymbol{\\mathscr{N}}} \\textbf{I}\n",
    "\\end{equation}\n",
    "\n",
    "Which causes simplfication of the above likelihoods to the following:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbb{P}_{\\textnormal{white}} = \\frac{1}{[2\\pi\\sigma]^{K/2}} \\exp \\bigg[ -\\frac{1}{2\\sigma^2} \\sum^K_i [X-\\mu(\\boldsymbol{\\theta})]^2_i\\bigg]\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\log[\\mathbb{P}_{\\textnormal{white}}(\\textbf{X}|\\boldsymbol{\\theta})] = \\frac{1}{2\\pi\\sigma} \\sum^K_i\\bigg[ [X-\\mu(\\boldsymbol{\\theta})]^2_i \\bigg] - \\frac{K}{2} \\log[{2\\pi\\sigma^2}]\n",
    "\\end{equation}\n",
    "\n",
    "We should note that this is **not** a suitable model of the noise at LIGO [2]. Implementing it nevertheless follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c938db2-25be-474a-9ebd-e83cc87422ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Global params ###\n",
    "\n",
    "fr = 100 # the number of frequency bins we'd have\n",
    "n = 10 # number of example samples of noise\n",
    "\n",
    "### FIRST FIND COVARIANCE - This is simpler as we can just take the variance of each element ###\n",
    "\n",
    "noise_samples = np.ones([n,fr]) # dummy array\n",
    "cov = np.var(noise_samples.flatten())*np.identity(fr)\n",
    "\n",
    "### We might then test this against some data... ###\n",
    "\n",
    "data = np.ones([1,fr]) \n",
    "model = np.ones([1,fr]) # dummy arrays we might observe from an experiment.\n",
    "\n",
    "def lhood_white(X, MU, C):\n",
    "    K = len(X)\n",
    "    sigma = C[0,0]\n",
    "    n = X-MU\n",
    "    return 1/(2*np.pi*sigma)**(K*0.5) * np.exp(-0.5*np.sum((n/sigma)**2))\n",
    "    \n",
    "def log_lhood_white(X,MU,C, ret_const=True): # we may also want to define the log-likelihood...\n",
    "    K = len(X)\n",
    "    sigma = C[0,0]\n",
    "    const = (-0.5*K*np.log((2*np.pi*sigma**2))) if ret_const else 0\n",
    "    n = X-MU\n",
    "    return np.sum(n**2/(2*np.pi*sigma**2)) + const\n",
    "    \n",
    "### Which would be implemented like ###\n",
    "\n",
    "lhood_white(data, model, cov) #or\n",
    "log_lhood_white(data, model, cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695e6621-1ac2-4952-bdcb-0e8500632c58",
   "metadata": {},
   "source": [
    "### What Happens When...?\n",
    "\n",
    "Let's consider some modifications. Firstly when we apply a scaling vector to the noise, $\\boldsymbol{\\alpha}$. We then get\n",
    "\\begin{equation}\n",
    "\\textbf{X} = \\boldsymbol{\\mu}(\\boldsymbol{\\theta}) + \\boldsymbol{\\alpha} \\boldsymbol{\\mathscr{N}}\n",
    "\\end{equation}\n",
    "Such that relation transforms to \n",
    "\\begin{equation}\n",
    "\\boldsymbol{\\alpha}^{-1}[\\textbf{X} - \\boldsymbol{\\mu}(\\boldsymbol{\\theta})] = \\boldsymbol{\\mathscr{N}}\n",
    "\\end{equation}\n",
    "In the static noise example this therefore us with an additional standard deviation on each element of the noise, so it follows that $\\boldsymbol{\\alpha}$ scales the power spectral density on each bin:\n",
    "\\begin{equation}\n",
    "\\mathbb{P}_{\\textnormal{stat}} = \\frac{1}{[\\prod^K_i 2\\pi\\sigma_i]^{1/2}} \\exp \\bigg[ -\\frac{1}{2} \\sum^K_i \\frac{[X-\\mu(\\boldsymbol{\\theta})]^2_i}{\\sigma^2_i \\alpha^2_i} \\bigg].\n",
    "\\end{equation}\n",
    "\n",
    "Instead of the gaussian example, let us consider a Poisson distribution. The single event probability of some noise $\\boldsymbol{\\mathscr{N}}_i$ follows \n",
    "\\begin{equation}\n",
    "\\mathbb{P}(\\boldsymbol{\\mathscr{N}}_i)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e400a56d-55c1-4c19-a365-512024c2e038",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
